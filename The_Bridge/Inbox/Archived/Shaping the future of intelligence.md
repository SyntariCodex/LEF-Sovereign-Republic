AGI Unbound with Kristinn Thórisson: The Hard Truth About Building Real Intelligence  
About this interview — Kristinn Thórisson

In this episode, Kristinn Thórisson—professor at Reykjavik University and director of the Icelandic Institute for Intelligent Machines—explains why true AGI requires more than scaled neural networks. He outlines key ingredients such as cumulative learning, causal reasoning, novelty-handling, transparency, and goal systems that can be inspected and understood. Thórisson argues that black-box AI cannot deliver safe or scientific progress and critiques short AGI timelines as fundamentally misguided. 

About Professor Kristinn Thórisson

Dr. Thórisson has been researching artificial intelligence for two decades, in academia and industry. His research centers on real-time interactive intelligences, complex cognitive systems, and mind models. At MIT, he pioneered new ideas in the area of communicative, multimodal intelligent agents. Recent projects include developing a cognitive architecture for the humanoid robot ASIMO by Honda Motor Corporation. He is the co-founder of CADIA, Iceland's first AI lab, and Radar Networks, a Semantic Web company in San Francisco. He has taught advanced AI courses at Columbia University, KT,H and Reykjavik University, and consulted for NASA and British Telecom, among others.

https://www.youtube.com/watch?v=E8VLAqaxBrk  
Interview Transcript  
\[Music\]  
0:06  
My name is Chris Thoren. I'm a professor here at Raiki University and I am the director of the Icelandic Institute for  
0:12  
Intelligent Machines. My main job is to research the phenomenon of general  
0:19  
intelligence and in a way uh that is intended to be suitable to be  
0:24  
implemented in computers. And um I also do a lot of applied AI and  
0:30  
uh at at the present time we we don't have any real AGI of general  
0:36  
intelligence in machines. So um the applied work involves a lot of  
0:43  
application of current knowledge, current theories and current technologies in various forms for  
0:50  
various purposes.  
0:58  
So um in engineering when you want to build a bridge or or skyscraper or something  
1:06  
um you start with listing the requirements you know it's got to withstand winds of certain strength and  
1:13  
and it's got to carry carry certain weights and so on. So uh this is not a very traditional  
1:20  
uh way to go about basic research but we have found that for intelligence because  
1:26  
it's a very complex phenomenon that um there are still no accepted theories for  
1:34  
uh it's a very good way to to to make progress on the hard questions of  
1:40  
general intelligence. So um we have essentially distilled  
1:47  
uh certain properties and features of general intelligence that that my team  
1:52  
and I are are are working on and have been working on for the past 15 20 years. And um and this includes for  
1:59  
instance um um cumulative learning. You you've got the the system has to be  
2:07  
situated. So uh because um uh Being an agent in the physical world or any world  
2:14  
for that matter that is not fully known um requires you to acquire knowledge on  
2:20  
the fly on the go and uh we want systems that can do that at all time at any time  
2:28  
just like animals including humans. And so cumulative  
2:33  
learning and cumulative reasoning this is one of the things that somewhat  
2:38  
separates uh the human species from other species but um uh and we like to think that  
2:45  
we're a little bit better at it or certainly we do have some abilities of reasoning that other species don't seem  
2:52  
to possess. Um, one of the telling differences is that we are have a much  
3:00  
much more sophisticated uh, communication system uh, given you know  
3:05  
natural what we call that natural language than other species. Uh, although other species have been able to  
3:12  
pick up some of that especially the syntax is um is very  
3:17  
difficult for other species to learn. it is fairly complicated and um for them to  
3:23  
to learn. So um so these are some requirements. So reasoning cumulative  
3:28  
learning um situatedness we we have to  
3:34  
um we we have to assume that the regularities in the world are due to  
3:41  
causal relations and uh causal relations may be hypothetical. uh it may be that  
3:48  
there are actually no causal relations in the universe and it's all statistics but certain things are more reliable  
3:55  
than others for getting things done and that's what we call um that's a practical view on causation. So if I  
4:02  
push this button and the light comes on, I know it came on because I pushed the button because I know how the mechanism  
4:08  
behind that system works. And if it doesn't come on, I can also say that  
4:13  
it's broken because I know how that system works. And so I know there's a cause effect relation. And I and I do  
4:21  
model that. And and and we do this and everyone every human does this. And animals too for for a a huge number of  
4:28  
things. And this is how we are able to generalize. So um one if you break down  
4:36  
reasoning for instance you know there there is prediction u there's uh abduction which is the inference of of  
4:43  
cause um the there is generalization um or or  
4:49  
uh induction and then there is analogy and these are extremely important  
4:55  
features for uh for general intelligence. um you have to have them all and they have to be systematic  
5:01  
because the because a a general intelligence is essentially modeling  
5:07  
um a rule-based system and when it's not rulebased we call it noise and uh we try  
5:13  
to break down and engineers try to break down the noise to find some signal in the noise but uh sometimes this is not  
5:20  
possible and then sometimes you have to go deeper or you have to zoom out um using microscopes or or telescopes or  
5:27  
whatever it is um and uh this has been the history of science for um for  
5:34  
centuries.  
5:43  
To me, the most convincing attempts at uh at creating AGI systems are holistic  
5:52  
approaches. That is uh the approaches that um try that attempt to have a complete list  
6:01  
of requirements from the very beginning and u or at least uh uh yeah attempt to  
6:09  
have it, you know, because you can't really ever be sure until you actually have achieved it. But um but the attempt  
6:17  
is is worth its weight in gold. And so these um these approaches uh essentially  
6:23  
include uh these requirements that I already mentioned and put them together in in a system that works as a whole  
6:31  
that is um that is implementable as a software system and that that's actually  
6:38  
um we we of course don't know if what uh what natural brains do is implementable  
6:44  
as a software system. We don't know that yet for sure, but it's very likely or at least we get we could get very very  
6:50  
close. And so that is um that is a test in fact of the quality of the ideas, how  
6:57  
far they are from being able to be implemented as software.  
7:08  
So it helps to have a good definition and we would call it working definition  
7:15  
of intelligence. Um there is danger in premature definitions of any strange or un or ill  
7:24  
understood phenomenon. So uh you got to be careful with that. But um we are  
7:30  
following a definition that Pay Wang u created in the mid '90s I believe and  
7:35  
did a really good paper on in 2020 which uh defines uh the which which defines  
7:43  
general intelligence as the ability to handle novelty and to uh to deal with the unexpected and figure stuff out. And  
7:51  
I my my version of that um is discretionary discretionarily  
7:58  
uh controlled adaptation uh under insufficient knowledge and  
8:03  
resources. So because no one knows what the future holds, we can never be sure that we know  
8:10  
what's going to happen or whether what we plan to do is going to work out ever. That's like a permanent state. So  
8:16  
essentially the future is always unknown. has some un unknown elements and um this definition defines  
8:24  
intelligence as the ability to handle that scenario that state. Um this this  
8:31  
definition essentially uh allows you to um to create new kinds of tests that  
8:38  
involve novelty. In fact, but it's not novelty as in no one's ever seen that before, but novelty to the system.  
8:47  
And I believe this this is this is a more fruitful way than any other that's ever been suggested. uh in fact it is  
8:54  
inherent in some of these ideas of prior like uh for instance uh Wnjak's uh  
9:00  
coffee test that you know a robot  
9:05  
could have could might be called to have general intelligence if it can go into  
9:11  
any uh any kitchen in any house on the street  
9:18  
that you live or you know in the city and make coffee. uh because no kitchen is exactly the same. U although this  
9:25  
does not get around the problem that you know this would be a coffee making robot  
9:31  
and and you wouldn't actually say that it it has general intelligence if all it can do is make coffee in any kitchen in  
9:37  
the city, you know. So it doesn't quite get it but it's sort of pointing in the right direction.  
9:50  
Yeah. So that's where where you get to a very important question of transparency. I if the if the systems we build  
9:57  
continue to be like LLMs and and other artificial neural network approaches um  
10:03  
I don't think we'll ever get to well actually it won't be good science and we  
10:08  
will never get to safe systems. this the the the trick to safety  
10:13  
uh as far as it can be done because the future is unknown and you know you would like these systems to be able to handle  
10:19  
unknown situations. Uh the only way to to get to some sort of safety is by the  
10:25  
transparency of their mechanisms and uh that means they're also their implementation and their actual state at  
10:33  
any point in time. So when you ask for instance a friend of yours um about you  
10:40  
know what their intention is and uh whether they think they can drive this car um and and your friend you you can  
10:47  
you have some experience with with with this person so you know you can trust them when they say uh when they give you  
10:53  
the answer and uh you can put your own judgment on what they're what they're saying and what they're planning to do  
10:58  
right um this is the kind of transparency I I think we we can actually achieve with AGI systems uh  
11:06  
with the added possibility of pressing pause and opening the hood and looking inside. Now that can only be done if  
11:13  
it's an ocean of a billion zeros and ones. There's no hope uh because the  
11:19  
semantics the meaning of the state of the system is completely hidden. Not only is it hidden to us, it's hidden to  
11:25  
the to the system itself. So you can't even engineer in any mechanism for the  
11:30  
system to ensure its own safety. um these systems uh you know a future of  
11:35  
robots driven by LMS is is actually a horror story in the making because uh  
11:41  
you can't ever trust these systems. You can't ever open the hood and be sure of anything and you can't ask them to  
11:47  
explain themselves. So um we must drive towards and in fact that's how science  
11:54  
works. It works by explaining things and we want to explain general intelligence.  
12:02  
We don't want just to be able to make a black box that does really cool stuff and nobody knows why. That's not  
12:09  
science. And that might be practical only for very very limited things u like  
12:14  
movie recommendations where no one's going to die. But most of the things that we want done in the world uh by  
12:20  
machines that aren't being done in in the world um require some sort of a judgment on the actions that are taken  
12:27  
due to the unpredictability of the future. So um transparency is really the  
12:33  
key here. So one of the things with respect to safety that is very important and with  
12:40  
respect to transparency is that uh the goals of the system that we build um are  
12:48  
uh can be inspected and the the and an AGI system of course or a semi- agi  
12:54  
system must be able to create its own goals and sub goals. It must be able to  
13:00  
um check its goals and verify them and run them past someone if it's unsure  
13:05  
about whether they're a good idea or not. And they must be concrete in the sense that a goal should be describable  
13:13  
in some way by some milestones. Like if I want to get beer from the fridge, I'm  
13:18  
going to go over there and then I'm going to reach out with my arm and open the door and then I'm going to scan, you  
13:25  
know, etc., etc. Um so uh well these are physical things there's also of course  
13:31  
cognitive things the system has to also be able to inspect its own state mental  
13:37  
state to some extent and to be able to reflect on that and learn about that just as it learns about anything else.  
13:44  
And this um requires uh transparent the generation and management of goals that  
13:50  
are transparent not just to the system itself but also to us. So it can it can  
13:56  
transfer any part of its mental state that's relevant at any point in time  
14:01  
into language or into some other form and communicate that to anyone anywhere.  
14:07  
Um, of course it's electronic so it's it's an AI, it's a robot, so it can send it, you know, somewhere, etc. And this  
14:14  
will allow us to make very very different systems that are way more safe. Um um and I I should add as a  
14:22  
footnote you know and this cannot be done without knowledge of cause effect relations.  
14:36  
The only way to really unify these things is to think of them all up front.  
14:41  
Um the this is not t talked about so much but uh every in in computer science  
14:47  
and and AI but every engineer knows that uh if you want to retrofit some major  
14:54  
functionality into an existing system it can hardly ever be done certainly not  
15:00  
properly. Um, if you design an airplane that actually flies and now, you know,  
15:05  
afterwards you have the thought, wouldn't it be cool if it could also dive, you know, and yeah, but how are  
15:13  
you going to retrofit that into a current design of an airplane that that's built on certain principles and  
15:19  
certain requirements, right? It is impossible. And one of the things that's impossible to retrofit is for instance  
15:27  
uh the into AI systems is the ability to handle time. If you don't design a  
15:32  
system, if you design a timeless system that has no concept of time and no special uh management of time, u you  
15:41  
look around you, you know, time flies, especially if you're having fun. So  
15:47  
it you're not going to be able to add it later. And LLMs, of course, being the, you know, AI dour um are completely  
15:55  
timeless. Uh yes, there will be a lot of naysayers here and and but they will all be wrong.  
16:02  
It's too long to go into uh it is actually still a discussion uh in  
16:08  
in the theoretical realm. But uh on the point of uh unifying these things in in  
16:14  
a in a single system, it's it's all about architecture. And just like if you  
16:20  
rip out the carburetor or or you know the gas tank or whatever uh from an internal combustion engine, it's just  
16:26  
not going to run. And you can talk and talk and talk about how it would be cool if it did have, you know, the spark  
16:32  
plugs in or if it did have some gas, etc., but no one's going to be convinced until you actually get one together and,  
16:40  
you know, you start it up and you drive. Um so, so it's very much like that, you  
16:45  
know. Um, the mind is a system of parts that all need to work together in  
16:51  
certain way to achieve a result. And it's a very comp it's the most complex  
16:57  
machine probably that science has tried to build.  
17:09  
All all AGI claims I view skeptically. And the reason is that uh the well  
17:17  
certainly most of the people who are let's face it only a fraction of the  
17:23  
people talking about AGI are actually working on AGI or have some experience in AGI. So everyone else who's talking  
17:30  
about AGI just doesn't know what they're saying. Just doesn't know doesn't understand the subject. and and yet they  
17:36  
might think they understand the subject and they they might know a lot about other things. Um but they would have to  
17:43  
prove to me that or you know convince me that those things are relevant to AGI.  
17:48  
Uh because I've spent uh most of my professional career actually trying to  
17:54  
figure out what matters and what doesn't matter in AGI. Now, I didn't used to have the G in there because I didn't  
17:59  
think it was necessary because to me AI was about general intelligence, but  
18:05  
apparently that that was I was in the major minority there. The majority of people were just pursuing whatever was  
18:11  
cool. So, most claims about AGI are um are really illfounded or unfounded. And  
18:19  
uh uh my favorite one is AGI is going to happen next year or just you know it's  
18:25  
or or or in five years. Uh these are very common numbers you get also within  
18:32  
a decade. Those are all baloney I'm afraid. Uh because and and the reason I  
18:39  
know that is because uh I have looked at the requirements for how to achieve what general  
18:47  
intelligence achieves. you know, even a short list of the things that almost  
18:52  
everyone agrees would be part of what a general intelligence can do, not not the  
18:57  
long list like, oh, our hopes and dreams of of what an AGI of the future might be. Um, is is so  
19:06  
um it requires such an intricate set of highly sophisticated mechanisms that  
19:13  
some of which are very close to philosophy still. And so that requires  
19:19  
research, that requires lab work, that requires experimentation and comparisons and so on. Uh that's just not going to  
19:26  
happen in a decade. It isn't. Um I think all of those claims are are based on on  
19:32  
a linear extrapolation of the feeling that people in tech have about the last  
19:38  
three years. Very bad basis to go from  
19:49  
So this is a this is a tough question. um uh we really don't have and when I  
19:57  
say we I mean science doesn't really have a good theory of of tasks  
20:02  
and uh we don't have um any way of saying that uh one task that's  
20:09  
reasonably different from another task is somehow equivalent when it comes to intelligence. So uh you know you there  
20:16  
is no way to say doing that task learning that task from scratch is  
20:21  
equivalent to learning that this task from scratch. There is no mechanism to compare tasks. Not I'm not I'm not even  
20:28  
saying you know there's no theories or good theories about intelligence or general intelligence. I'm saying there  
20:34  
is no theory of tasks which seems like a ridiculous state of of uh of knowhow.  
20:41  
And uh what my team has been doing uh sort of on the side because our main  
20:46  
thing is AGI is to to look at that question and it has actually helped us  
20:51  
quite a bit and informed us in how we think about uh AGI.  
20:56  
So um with respect to generality um  
21:02  
I' I'd like to replace it with novelty because that seems to be uh easier. If  
21:08  
you have a system that's never seen X and you're presented with an X or a small X, you know, that is much more um  
21:16  
promising as a as a way to assess whether there's some generality there or  
21:22  
not. You know, how how well can the system generalize from one case to another from one knowledge set to  
21:29  
another. Uh in fact, you can actually boil it down to the question, how does  
21:34  
the system make new knowledge? And if it is anything like humans, we  
21:42  
think that it involves uh coming up with ideas and trying them out. But before  
21:49  
you try them out in the physical world, there's thinking. And when you think about this those things and how you came  
21:56  
about the new new ideas and how you evaluated them in your head before  
22:01  
trying them out. If you use that even if you're completely wrong and it never works if you use that experience to  
22:10  
learn how to do that now you're you have a system that can actually learn to  
22:15  
learn. And I I believe that is one of the requirements uh of general intelligence.  
22:21  
You have to be able to reflect on your own knowledge consciously or subconsciously. You know,  
22:28  
don't don't think too much about how it happens in your head because we're we're just talking about mechanisms here.  
22:33  
After all, this is going to have to be written as software, right? Uh to to  
22:38  
essentially improve your own uh ability to deal with novelty.  
22:53  
Okay, I'm gonna I'm going to go with a nice surprise. Actually there have been  
22:58  
several but the big the one big nice surprise uh happened uh about 15 years  
23:04  
ago when when we formulated this my team and I this this approach uh that I just  
23:10  
outlined and um and the initial um outcome of of applying that approach  
23:17  
that methodology to creating a system um we had come up with a very lofty  
23:25  
uh demonstration that we thought would we would never achieve. And this  
23:30  
demonstration involved a system that could learn to do a TV interview and learn the language she being used  
23:37  
without having been given anything any knowledge except two pages of very  
23:43  
sparse code. So really learning a novel task from scratch on its own. Uh and we  
23:50  
thought, you know, if we can get halfway through this, uh we're we're that would that would be awesome. we on the right  
23:56  
track, right? Well, it turned out that the task we gave this system when when we had built it was too simple. It  
24:03  
learned to do uh a fiveminute interview on recycling of six different materials  
24:09  
by watching humans and and it wasn't, you know, 2 million hours or or 200,000  
24:16  
hours or or or 2,000 hours. It was 20 hours  
24:22  
on a supercomput. know on a laptop from 2012\.  
24:31  
So that was, you know, that that I don't think I'll ever see another nicer price  
24:37  
like that in my career. In fact, that was just wildly out of out of the ballpark. So o open era.org or um era  
24:47  
spelled a e r autonomous  
24:52  
um reflective uh a e autonomous empirical reasoning  
24:59  
system  
25:13  
right now It's uh having uh this effect that um a lot of people like to dream  
25:22  
about the near and far future. There's much more dreaming now especially you  
25:27  
know um both in social media but also you know in in in the media in general  
25:34  
people allow themselves to to imagine all sorts of future scenarios machines  
25:39  
that don't exist now and they invariably think they are uh not far away not many  
25:46  
years away uh under a decade um  
25:51  
the Um the trend seems to be that people are in  
25:58  
in the sort of contemporary AI tradition which is the applied AI.  
26:06  
Um by far the biggest attention is on applied AI but academic research on AI  
26:12  
still needs to happen and is still ongoing and of course is the source of  
26:18  
what we have now. The applied AI now took essentially 60 years. This is something that never gets  
26:25  
reported. You know, most of the demos we see are like okay, this was two years in the making. No, it was actually and very  
26:32  
clearly uh 60 years ago that it was uh I believe it was in 52 that or 53 that the  
26:39  
first artificial neural networks paper was was published. So people have been working on this um but it's not counted  
26:46  
because it didn't enter um industry until five you know 10 years ago and so  
26:52  
so there's a misconception there. So um all of the fundamental ideas still have to be created in uh by basic research  
27:01  
and um the um the sooner that  
27:06  
um that the general sort of let's say industry those researchers in industry  
27:13  
uh who are working on contemporary AI and applied AI uh the sooner they realize the limitations of that  
27:19  
technology that it can't be linearly extrapolated uh you forever into the  
27:25  
future. And it's not just about scale, the sooner the the work that we do in AGI is going to have an impact because  
27:32  
they're going to go then and look what have people done. Uh because that takes a lot of work and uh it's easy to uh to  
27:40  
dismiss that. Um it's it's part as as Edison said, it's part uh in uh  
27:47  
inspiration, part perspiration. Um but u there it's a cycle and it's not just one  
27:55  
99% perspiration 1% inspiration as he said or as the famous quote goes but  
28:01  
rather a cycle where you you know you have periods of just uh hard work and  
28:08  
paper writing and so on and then you you have an epiphany uh or a small epiphany or a series of small ones over a period  
28:15  
of one or two years and uh then you lift it to the next level uh and that's how  
28:20  
basic research goes. But that's the only way to produce fundamental new ideas and  
28:27  
technologies. Um when they enter industry is another it's a very different question. You know  
28:34  
there it has to be accessible enough. There has to be sufficient documentation. It has to be runnable on  
28:40  
on on the machines that are available and so on. So there's a lot of of of of  
28:45  
parameters there that have nothing to do with the state of the knowledge per se.  
28:50  
Uh these are two separate uh things that uh or mechanisms in fact that have that  
28:56  
play together and uh until about 10 years ago industry completely ignored AI as a as a as more  
29:05  
or less a joke.  
29:14  
AI is um is highly dependent on engineering clearly. Um the field has  
29:23  
essentially decided that part of its mission at least 50%.  
29:29  
Is to make machines that can do stuff that no machine could do before and  
29:34  
specifically machines that think or do something along those lines.  
29:40  
um it's not to copy humans, it's to do  
29:45  
things that require thinking or something along those lines. So, uh, but for the most part, uh, AI  
29:52  
has has, um, maxed that principle out, uh, um, and, uh, paid less attention to  
30:03  
its own detriment. I I believe to the scientific pursuit, which is not  
30:08  
engineering. Um, the scientific pursuit has to do with identifying what knowledge is  
30:14  
missing and filling that gap. And part of the part of that knowledge is how to  
30:20  
put together a whole system that can actually be more autonomous uh be more  
30:26  
reliable, be more trustworthy, etc., etc. Um, uh, that's that's the science  
30:32  
part. So uh I'm not going to tell people which side to pick  
30:39  
but the progress of AI cannot possibly depend only on one or  
30:46  
the other of these two. So just be careful that the field isn't  
30:51  
uh so lopsided that it hampers progress.  
30:56  
If you really believe that you all you need to do is scale up LLMs to get AGI  
31:02  
then it this is going to be over very soon but we already see the evidence you  
31:07  
know it's accumulating now for the past two years there's a plateau and they're not going to scale any further and  
31:14  
there's a fundamental reason for that I could have told them that 10 years ago five years ago if they had listened  
31:21  
um but uh investors keep pouring money into that um on on broken promises  
31:27  
basically. And um so you have to have  
31:32  
informed decision- making in how you spend the money if you want to really make progress. If you really want  
31:39  
safety, if you don't care about safety, you know, then um  
31:45  
if you work for someone who who has that mindset, go find another job. That that would be my second message.  
31:52  
Please don't uh add to the confusion and u  
31:58  
um breakdown of society due to AI. You know, don't let that be our legacy. Let  
32:04  
it let it be the legacy of understanding general intelligence and be having so good theories that we can actually make  
32:10  
safe systems.  
32:19  
Oh yes. So I'm always looking for collaborations. Um I am especially interested in getting collaborations  
32:27  
uh on people that share the assumption that you need to have a lot of  
32:33  
ingredients to make some progress. Um yes it sometimes sounds daunting. It's  
32:39  
like why can't we just cut this out and that out and people open the AAMS razor and they start cutting away and and then  
32:46  
suddenly you know the you were thought you were studying the tree but now it has no leaves and it's not really a tree  
32:53  
anymore. It's some abomination of a of a butchered phen natural phenomenon. The  
33:00  
the best example we have of general intelligence is human intelligence. So  
33:05  
let's not slice it so thin that every slice looks like something very very different. We have to have at least like  
33:13  
a minimum of of the required ingredients to start working and there are and there  
33:19  
are all sides to that uh um to to the questions that come from that. So now  
33:25  
let's say you have these ingredients you know you take the broad view not the deep view you know if you have to pick  
33:33  
um and so take the broad view think about how they work together and now you  
33:40  
can bring in a a background from cognitive science from computer science from neuroscience from mathematics  
33:47  
and um any one of these backgrounds working  
33:53  
together with the right questions can actually has a great chance of  
33:59  
advancing our understanding of general intelligence. So that you know it's it's a fairly  
34:06  
general call for for for collaboration.  
34:17  
\[Music\]  
All  
For you  
Recently uploaded  
